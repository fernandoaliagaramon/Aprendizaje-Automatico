function [ mejor_tam, mejor_error ] = kfold( n, k , Xdatos, ydatos Xtest, ytest)

mejor_tam = 0;
mejor_error = 100000;

errores_T = [];
errores_V = [];



for grado = 1:n
    error_T = 0;
    error_V = 0;
    [Xexp] = expandir (Xdatos, [5 grado 1]);
    [ Xn, mu, sig ] = normalizar( Xexp )
    for i = 1:k
        [ Xcv, ycv, Xtr, ytr ] = particion( i, k, Xn, ydatos );
        h = Xtr\ytr;
        error_T = error_T + RMSE (h, Xtr, ytr);
        error_V = error_V + RMSE (h, Xcv, ycv);
    end
    error_T = error_T / k;
    error_V = error_V / k;
    errores_T = [errores_T error_T];
    errores_V = [errores_V error_V];
    if (error_V < mejor_error )
        'Actualizo mejor error y tamaño.'
        'Nuevo mejor tamaño: '
        mejor_tam = grado
        'Nuevo mejor error: '
        mejor_error = error_V
    end
end



figure;
grid on; hold on;
title(sprintf('Rojo -> errores entrenamiento. Azul -> errores validacion'));
plot(errores_T,'r')
plot(errores_V)
mejor_tam
mejor_error

[Xexp] = expandir (Xdatos, [5 mejor_tam 1]);
[ Xn, mu, sig ] = normalizar( Xexp );
h = Xn\ydatos;
[ wdes ] = desnormalizar( h, mu, sig )

%RMSE (wdes, Xtest, ytest);

[ mejor_tam, mejor_error ] = regularizacion ( 10, 10 , Xdatos, ydatos);

[Xexp] = expandir (Xdatos, [10 5 5]);
[ Xn, mu, sig ] = normalizar( Xexp );

th = Xn \ ydatos;

[x] = expandir (Xtest, [5 6 1]);
[ Xn, mu, sig ] = normalizar( x );

RMSE (th, Xn, ytest)